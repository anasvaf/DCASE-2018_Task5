{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple MLP VAE in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "X_train: (873, 257, 251, 1)\n",
      "X_val: (888, 257, 251, 1)\n",
      "y_train: (873, 10)\n",
      "y_val: (888, 10)\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.load('total_numpys/total_spectrograms.npy')\n",
    "X_train = np.load('spectrograms_fold_1.npy')\n",
    "X_val = np.load('spectrograms_fold_2.npy')\n",
    "# y_train = np.load('total_numpys/total_labels.npy')\n",
    "y_train = np.load('labels_fold_1.npy')\n",
    "y_val = np.load('labels_fold_2.npy')\n",
    "\n",
    "n_train_samples = X_train.shape[0]\n",
    "n_valid_samples = X_val.shape[0]\n",
    "n_classes = 10\n",
    "y_train_one_hot = np.eye(n_classes)[y_train]\n",
    "y_valid_one_hot = np.eye(n_classes)[y_val]\n",
    "\n",
    "\n",
    "print('Dataset sizes:')\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_val:', X_val.shape)\n",
    "print('y_train:', y_train_one_hot.shape)\n",
    "print('y_val:', y_valid_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to the graph \n",
    "rows = 257\n",
    "cols = 251\n",
    "n_pixels = rows*cols\n",
    "X = tf.placeholder(tf.float32, shape=([None, n_pixels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer creation functions\n",
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def FC_layer(X, W, b):\n",
    "    return tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "The encoder consists of a 2 layer, fully connected feed forward network that reduces the dimensionality from the original number of features (e.g. pixel) to the dimensionality of the latent space. This network has two outputs -- the mean and (log) standard deviation of a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "latent_dim = 20\n",
    "h_dim = 500\n",
    "\n",
    "W_enc = weight_variable([n_pixels, h_dim], 'W_enc')\n",
    "b_enc = bias_variable([h_dim], 'b_enc')\n",
    "# tanh activation function to replicate original model\n",
    "h_enc = tf.nn.tanh(FC_layer(X, W_enc, b_enc))\n",
    "\n",
    "W_mu = weight_variable([h_dim, latent_dim], 'W_mu')\n",
    "b_mu = bias_variable([latent_dim], 'b_mu')\n",
    "mu = FC_layer(h_enc, W_mu, b_mu)\n",
    "\n",
    "W_logstd = weight_variable([h_dim, latent_dim], 'W_logstd')\n",
    "b_logstd = bias_variable([latent_dim], 'b_logstd')\n",
    "logstd = FC_layer(h_enc, W_logstd, b_logstd)\n",
    "\n",
    "# reparameterization trick\n",
    "noise = tf.random_normal([1, latent_dim])\n",
    "z = mu + tf.multiply(noise, tf.exp(.5*logstd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "The decoder is also a feedforward, fully connected network -- however, it goes from the dimensionality of the latent space back to the original dimensionality of the data (e.g. pixels). The output of the network is squashed between 0 and 1 with a sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "W_dec = weight_variable([latent_dim, h_dim], 'W_dec')\n",
    "b_dec = bias_variable([h_dim], 'b_dec')\n",
    "h_dec = tf.nn.tanh(FC_layer(z, W_dec, b_dec))\n",
    "\n",
    "W_reconstruct = weight_variable([h_dim, n_pixels], 'W_reconstruct')\n",
    "b_reconstruct = bias_variable([n_pixels], 'b_reconstruct')\n",
    "reconstruction = tf.nn.sigmoid(FC_layer(h_dec, W_reconstruct, b_reconstruct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "Each of these outputs is taken to be the mean of a Bernoulli distribution (in this example, a Bernoulli distribution is appropriate because our data is binary). The variational lower bound is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{z\\sim Q(z|X)}\\log P(X|z) - D(Q(z|X)||P(z))\n",
    "$$\n",
    "When $P$ is a Bernouli distribution, the log likelihood is given by\n",
    "\n",
    "$$\n",
    "\\log P(X|z) = \\sum_i^N X^{(i)}\\log y^{(i)} + (1 − X^{(i)}) \\cdot \\log(1 − y^{(i)})\n",
    "$$\n",
    "where $N$ is the number of training samples (in our case, the batchsize), and $y^{(i)}$ is the reconstruction from the latent code $z^{(i)}$. The KL divergence between a gaussian $Q$ with mean $\\mu$ and standard deviation $\\sigma$ and a standard normal distribution $P$ is given by:\n",
    "\n",
    "$$\n",
    "D(Q||P) = -\\frac{1}{2}\\sum_j^J \\big(1 + \\log((\\sigma_j)^2) - (\\mu_j)^2 - (\\sigma_j)^2\\big)\n",
    "$$\n",
    "We want to maximize this lower bound, but because tensorflow doesn't have a 'maximizing' optimizer, we minimize the negative lower bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variational lower bound\n",
    "\n",
    "# add epsilon to log to prevent numerical overflow\n",
    "log_likelihood = tf.reduce_sum(X*tf.log(reconstruction + 1e-9)+(1 - X)*tf.log(1 - reconstruction + 1e-9), \n",
    "                               reduction_indices=1)\n",
    "KL_term = -.5*tf.reduce_sum(1 + 2*logstd - tf.pow(mu,2) - tf.exp(2*logstd), reduction_indices=1)\n",
    "variational_lower_bound = tf.reduce_mean(log_likelihood - KL_term)\n",
    "optimizer = tf.train.AdadeltaOptimizer().minimize(-variational_lower_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Now all we have to do is run the optimizer until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (16, 257, 251, 1) for Tensor 'Placeholder:0', which has shape '(?, 64507)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-319dce0d514a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mrecording_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mvlb_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariational_lower_bound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1128\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1129\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (16, 257, 251, 1) for Tensor 'Placeholder:0', which has shape '(?, 64507)'"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "batch_size = 16\n",
    "num_iterations = 1000000\n",
    "recording_interval = 1000\n",
    "variational_lower_bound_array = []\n",
    "log_likelihood_array = []\n",
    "KL_term_array = []\n",
    "iteration_array = [i*recording_interval for i in range(num_iterations//recording_interval)]\n",
    "\n",
    "n_train_batches = n_train_samples // batch_size\n",
    "for i in range(num_iterations):\n",
    "    for i in range(n_train_batches):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "\n",
    "        x_batch = X_train[start:end]\n",
    "        \n",
    "        sess.run(optimizer, feed_dict={X: x_batch})\n",
    "        if (i%recording_interval == 0):\n",
    "            vlb_eval = variational_lower_bound.eval(feed_dict={X: x_batch})\n",
    "            print(\"Iteration: {}, Loss: {}\".format(i, vlb_eval))\n",
    "            variational_lower_bound_array.append(vlb_eval)\n",
    "            log_likelihood_array.append(np.mean(log_likelihood.eval(feed_dict={X: x_batch})))\n",
    "            KL_term_array.append(np.mean(KL_term.eval(feed_dict={X: x_batch})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(iteration_array, variational_lower_bound_array)\n",
    "plt.plot(iteration_array, KL_term_array)\n",
    "plt.plot(iteration_array, log_likelihood_array)\n",
    "plt.legend(['Variational Lower Bound', 'KL divergence', 'Log Likelihood'], bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.title('Loss per iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model:\n",
    "    saver.restore(sess, os.path.join(os.getcwd(), \"Trained Bernoulli VAE\"))\n",
    "\n",
    "num_pairs = 10\n",
    "image_indices = np.random.randint(0, 200, num_pairs)\n",
    "for pair in range(num_pairs):\n",
    "    x = np.reshape(mnist.test.images[image_indices[pair]], (1, n_pixels))\n",
    "    plt.figure()\n",
    "    x_image = np.reshape(x, (rows,cols))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(x_image)\n",
    "    x_reconstruction = reconstruction.eval(feed_dict={X: x})\n",
    "    x_reconstruction_image = (np.reshape(x_reconstruction, (rows,cols)))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(x_reconstruction_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
