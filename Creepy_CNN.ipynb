{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Basic CNN architecture in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy.misc import imsave\n",
    "import errno\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow import layers\n",
    "from tensorflow.contrib import distributions as dist\n",
    "from tensorflow.contrib import slim\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentdir = os.getcwd()\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(parentdir, 'UrbanSound8K/metadata/UrbanSound8K.csv'))\n",
    "metadata.drop_duplicates('class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the distributions of the classes (How balanced is the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['class'].value_counts().plot(kind='pie', \n",
    "                                  figsize=(8,6), \n",
    "                                  fontsize=13, \n",
    "                                  autopct='%1.1f%%', \n",
    "                                  wedgeprops={'linewidth': 5}\n",
    "                                  )\n",
    "plt.axis('off')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to dataset\n",
    "sound_dir = os.path.join(parentdir)\n",
    "\n",
    "# choosing one sound from each category\n",
    "unique_sounds = [\"14113-4-0-1.wav\",\"24074-1-0-0.wav\",\"15564-2-0-1.wav\",\"7383-3-0-0.wav\",\n",
    "                 \"57320-0-0-4.wav\",\"17592-5-1-0.wav\",\"7061-6-0-0.wav\", \n",
    "                 \"98223-7-0-0.wav\",\"40722-8-0-0.wav\",\"21684-9-0-5.wav\"]\n",
    "sound_names = [\"air conditioner\", \"car horn\", \"children playing\", \"dog bark\", \n",
    "               \"drilling\", \"engine idling\", \"gun shot\", \n",
    "               \"jackhammer\", \"siren\", \"street music\"]\n",
    "\n",
    "# raw time-series values for these sounds\n",
    "raw = []\n",
    "for u in unique_sounds:\n",
    "    ts, sr = librosa.load(os.path.join(sound_dir, u))\n",
    "    raw.append(ts)\n",
    "\n",
    "# waveplot of unique sound-classes within dataset\n",
    "def waves(raw_sounds):    \n",
    "    i = 1\n",
    "    fig = plt.figure(figsize=(12,15))\n",
    "    for file,name in zip(raw_sounds,sound_names):\n",
    "        plt.subplot(10,1,i)\n",
    "        librosa.display.waveplot(np.array(file), x_axis=None)\n",
    "        plt.title(name.title())\n",
    "        i += 1\n",
    "    plt.suptitle(\"Waveplot\", x=0.52, y=1.02, fontsize=12)\n",
    "    fig.tight_layout()\n",
    "\n",
    "waves(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sure_path_exists(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert each wav file to spectrogram image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# N_FFT = 512\n",
    "# HOP_LEN = N_FFT // 2\n",
    "# SR = 16000\n",
    "# fold_list = ['fold1', 'fold2', 'fold3', 'fold4', 'fold5', 'fold6', 'fold7', 'fold8', 'fold9', 'fold10']\n",
    "# exception_count = 0\n",
    "rows = 257\n",
    "cols = 251\n",
    "\n",
    "# oof = open('file_exceptions.txt', 'w') # redirect error exceptions for audio files\n",
    "\n",
    "# target_spectrogram_folder_images = \"UrbanSound8K/spectrograms\"\n",
    "\n",
    "# max_number_seconds = 4\n",
    "\n",
    "# for i in range(10):\n",
    "#     mypath = 'UrbanSound8K/audio/'+ fold_list[i] + '/'\n",
    "#     files = [f for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "\n",
    "#     make_sure_path_exists(target_spectrogram_folder_images + \"/\" + fold_list[i])\n",
    "\n",
    "#     for f in tqdm_notebook(files):\n",
    "#         fn = mypath + f\n",
    "#         try: \n",
    "#             data, rate = librosa.load(fn, sr=SR, mono=True)\n",
    "\n",
    "#             # Zero pad recordings that are less than 4 seconds\n",
    "#             if data.shape[0] != SR*max_number_seconds:\n",
    "#                 data = np.append(data, np.zeros((SR*max_number_seconds - data.shape[0], )))\n",
    "\n",
    "#             X = librosa.stft(data, n_fft=N_FFT, hop_length=HOP_LEN)\n",
    "#             D = librosa.amplitude_to_db(np.abs(X))\n",
    "#             D = np.flipud(D)\n",
    "\n",
    "#     #             plt.imshow(D, cmap='gray')\n",
    "#     #             plt.show()\n",
    "#             if D.shape[0] != rows or D.shape[1] != cols:\n",
    "#                 print(audio_name, D.shape)\n",
    "\n",
    "#             imsave(target_spectrogram_folder_images + \"/\" + fold_list[i] + \"/\" + f.split(\".wav\")[0] + '.png', D)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(os.path.basename(os.path.normpath(fn)), file=oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert spectrograms to NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(data):\n",
    "    \n",
    "    # Extract target class from the filepath\n",
    "    \n",
    "    label = data.split('-')[1]\n",
    "    return int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = os.path.join(target_spectrogram_folder_images, \"fold2\", \"14780-9-0-0.png\")\n",
    "# print(\"file path =\", example)\n",
    "# print('')\n",
    "# print(\"label:\", make_labels(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     spectrograms_array = []\n",
    "#     labels = []\n",
    "#     mypath = 'UrbanSound8K/spectrograms/'+ fold_list[i] + '/'\n",
    "#     files = [f for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "    \n",
    "#     for f in tqdm_notebook(files):\n",
    "#         image_file = mypath + f\n",
    "#         img = load_img(image_file, grayscale=True)\n",
    "#         x = img_to_array(img)\n",
    "#         x /= 255.\n",
    "#         spectrograms_array.append(x)\n",
    "#         labels.append(make_labels(image_file))\n",
    "#     np.save(\"spectrograms_fold_{0}.npy\".format(i+1), np.array(spectrograms_array))\n",
    "#     np.save(\"labels_fold_{0}.npy\".format(i+1), np.array(labels))\n",
    "# print(\"\\nAll images are converted to numpy arrays!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the Convolutional Variational Autoencoder !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.load('total_numpys/total_spectrograms.npy')\n",
    "X_train = np.load('spectrograms_fold_1.npy')\n",
    "X_val = np.load('spectrograms_fold_2.npy')\n",
    "# y_train = np.load('total_numpys/total_labels.npy')\n",
    "y_train = np.load('labels_fold_1.npy')\n",
    "y_val = np.load('labels_fold_2.npy')\n",
    "\n",
    "print('Dataset sizes:')\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_val:', X_val.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax.imshow(np.reshape(X_train[i], (257, 251)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_train_samples = X_train.shape[0]\n",
    "n_valid_samples = X_val.shape[0]\n",
    "n_classes = 10\n",
    "y_train_one_hot = np.eye(n_classes)[y_train]\n",
    "y_valid_one_hot = np.eye(n_classes)[y_val]\n",
    "\n",
    "# print(X_val[0:32])\n",
    "# (y_valid_one_hot[0:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few data augmentations (image-domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image(image, training):   \n",
    "    if training:\n",
    "        # Randomly crop the input image\n",
    "        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\n",
    "        # Randomly flip the image horizontally\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        # Randomly adjust hue, contrast and saturation\n",
    "        image = tf.image.random_hue(image, max_delta=0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "        # Limit the image pixels between [0, 1] in case of overflow\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "    else:\n",
    "        # Crop the input image around the centre so it is the same\n",
    "        # size as images that are randomly cropped during training\n",
    "        image = tf.image.resize_image_with_crop_or_pad(image,\n",
    "                                                       target_height=img_size_cropped,\n",
    "                                                       target_width=img_size_cropped)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def pre_process(images, training):\n",
    "    images = tf.map_fn(lambda image: pre_process_image(image, training), images)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "n_epochs = 30\n",
    "n_train_batches = n_train_samples // batch_size\n",
    "n_valid_batches = n_valid_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_placeholder = tf.placeholder(tf.float32, [None, rows, cols, n_channels])\n",
    "y_placeholder = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "input_layer = X_placeholder\n",
    "\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=input_layer,\n",
    "                        filters=32,\n",
    "                        kernel_size=[5,5],\n",
    "                        padding=\"same\",\n",
    "                        activation=tf.nn.relu)\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                               pool_size=[2,2],\n",
    "                               strides=2)\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                        filters=32,\n",
    "                        kernel_size=[5,5],\n",
    "                        padding=\"same\",\n",
    "                        activation=tf.nn.relu)\n",
    "\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                               pool_size=[2,2],\n",
    "                               strides=2)\n",
    "\n",
    "pool2_flat = tf.reshape(pool2, [-1, 32 * 62 * 64])\n",
    "\n",
    "fully_connected = tf.layers.dense(inputs=pool2_flat,\n",
    "                                 units=1024,\n",
    "                                 activation=tf.nn.relu)\n",
    "\n",
    "dropout = tf.layers.dropout(inputs=fully_connected,\n",
    "                           rate=0.4)\n",
    "\n",
    "logits = tf.layers.dense(inputs=dropout,\n",
    "                        units=n_classes)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                                labels=y_placeholder))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y_placeholder,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print()\n",
    "        print(\"Epoch\", epoch)\n",
    "        \n",
    "        for i in range(n_train_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            \n",
    "            x_batch = X_train[start:end]\n",
    "            y_batch = y_train_one_hot[start:end]\n",
    "            _train_step, _loss = sess.run([train_step, loss],\n",
    "                                        feed_dict=\n",
    "                                        {X_placeholder:x_batch,\n",
    "                                         y_placeholder:y_batch\n",
    "                                        })\n",
    "    \n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                _loss, _accuracy = sess.run([loss, accuracy],\n",
    "                                 feed_dict={\n",
    "                                     X_placeholder:x_batch,\n",
    "                                     y_placeholder:y_batch\n",
    "                                 })\n",
    "                \n",
    "                print(\"Minibatch loss:\" , _loss)  \n",
    "                print(\"Accuracy:\", _accuracy)\n",
    "          \n",
    "    print()\n",
    "    print(\"Optimization done! Calculating test error...\")\n",
    "    \n",
    "    # TESTING\n",
    "    losses = []\n",
    "    acc = []\n",
    "\n",
    "    for i in range(n_valid_batches):\n",
    "\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "\n",
    "        x_test_batch = X_val[start:end]        \n",
    "        y_test_batch = y_valid_one_hot[start:end]\n",
    "\n",
    "\n",
    "        test_loss, test_accuracy = sess.run([loss, accuracy],\n",
    "                                            feed_dict={\n",
    "                                                X_placeholder:x_test_batch,\n",
    "                                                y_placeholder:y_test_batch\n",
    "                                            })\n",
    "\n",
    "        losses.append(test_loss)\n",
    "        acc.append(test_accuracy)\n",
    "\n",
    "    print()\n",
    "    print(\"Average loss on test set is:\", np.mean(losses))\n",
    "    print(\"Accuracy on test set:\", np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
